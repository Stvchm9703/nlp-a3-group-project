# NLP Assignment 3 Groups 21

- Chansun Hwang
- Choonsik Cho
- Ho Man Cheng
- Leon Ambrose Lusbo
- Sunghyeok Jang

# NER Model Comparison Project

This project aims to compare the performance of five different Named Entity Recognition (NER) models using the CoNLL-2003 dataset. The goal is to provide a comprehensive comparison report that includes both performance metrics and resource requirements for each model.

## Objective
The main objective of this project is to evaluate and compare the effectiveness of various NER models on the CoNLL-2003 dataset. By examining metrics such as accuracy, precision, recall, F1 score, training time, and resource requirements, we aim to identify the strengths and weaknesses of each model.

## Dataset
The CoNLL-2003 dataset is utilized for this project. It consists of English news articles annotated with named entity labels, including persons, organizations, locations, and miscellaneous entities.

## Models
The following NER models are included in the comparison:
1. BiLSTM-CRF
2. Transformers based classifier
3. BERT

## Comparison Report
The comparison report will cover the following aspects for each model:
- Accuracy
- Precision
- Recall
- F1 Score
- Training Time
- Resource Requirements (e.g., memory usage, inference time)
- Robustness to Domain Variability
- Ease of Use and Integration
- Customization Flexibility
- Model Size and Footprint

## How to Run
To replicate the comparison, follow these steps:
1. Download the CoNLL-2003 dataset.
2. Install the necessary libraries and dependencies for each NER model.
3. Train and evaluate each model on the CoNLL-2003 dataset.
4. Record the performance metrics and resource requirements for each model.
5. Compile the results into a comparison report.
6. The respective model codes can be found in the alternatives folder.

## Results
The results of the comparison will be presented in the comparison report. This report will provide insights into the performance and suitability of each model for NER tasks, considering various evaluation criteria. The confusion matrix images are stored in the images folder, and classification reports are stored in the results folder

### Jupyter Notebook Reports
Each NER model will have its own Jupyter Notebook report, detailing the training process, evaluation results, and analysis. These reports will be structured to provide a clear overview of the model's performance and insights into its strengths and weaknesses.

### How to Run Notebooks
To view and run the Jupyter Notebook reports for each model, follow these steps:
1. Navigate to the corresponding notebook file for the desired model.
2. Open the notebook using Jupyter Notebook or JupyterLab.
3. Execute the cells sequentially to reproduce the training and evaluation process.
4. Review the analysis and results presented in the notebook.

## Future Work
Potential future work includes:
- Exploring additional NER models or variants.
- Evaluating models on other datasets to assess generalizability.
- Fine-tuning models on domain-specific data for improved performance.
- Optimizing resource usage and model efficiency.

